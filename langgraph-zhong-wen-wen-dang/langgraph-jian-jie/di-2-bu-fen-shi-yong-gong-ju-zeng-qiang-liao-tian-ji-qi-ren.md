# ç¬¬2éƒ¨åˆ†ï¼šä½¿ç”¨å·¥å…·å¢å¼ºèŠå¤©æœºå™¨äºº

ä¸ºäº†å¤„ç†æˆ‘ä»¬çš„èŠå¤©æœºå™¨äººæ— æ³•â€œæ ¹æ®è®°å¿†â€å›ç­”çš„æŸ¥è¯¢ï¼Œæˆ‘ä»¬å°†é›†æˆä¸€ä¸ªç½‘ç»œæœç´¢å·¥å…·ã€‚æˆ‘ä»¬çš„æœºå™¨äººå¯ä»¥ä½¿ç”¨æ­¤å·¥å…·æŸ¥æ‰¾ç›¸å…³ä¿¡æ¯å¹¶æä¾›æ›´å¥½çš„å“åº”ã€‚

### ä¾èµ–

åœ¨æˆ‘ä»¬å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨å·²ç»å®‰è£…äº†å¿…è¦çš„è½¯ä»¶åŒ…å¹¶è®¾ç½®äº†APIå¯†é’¥ï¼šé¦–å…ˆï¼Œå®‰è£…ä½¿ç”¨Tavilyæœç´¢å¼•æ“çš„è¦æ±‚ï¼Œå¹¶è®¾ç½®æ‚¨çš„TAVILY\_API\_KEYã€‚

```python
%%capture --no-stderr
%pip install -U tavily-python
```

```python
_set_env("TAVILY_API_KEY")
```

æ¥ä¸‹æ¥ï¼Œå®šä¹‰å·¥å…·ï¼š

```python
from langchain_community.tools.tavily_search import TavilySearchResults

tool = TavilySearchResults(max_results=2)
tools = [tool]
tool.invoke("What's a 'node' in LangGraph?")
```

\[{'url': 'https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141', 'content': 'Nodes: Nodes are the building blocks of your LangGraph. Each node represents a function or a computation step. You define nodes to perform specific tasks, such as processing input, making ...'}, {'url': 'https://js.langchain.com/docs/langgraph', 'content': "Assuming you have done the above Quick Start, you can build off it like:\nHere, we manually define the first tool call that we will make.\nNotice that it does that same thing as agent would have done (adds the agentOutcome key).\n LangGraph\nğŸ¦œğŸ•¸ï¸LangGraph.js\nâš¡ Building language agents as graphs âš¡\nOverview\u200b\nLangGraph is a library for building stateful, multi-actor applications with LLMs, built on top of (and intended to be used with) LangChain.js.\n Therefore, we will use an object with one key (messages) with the value as an object: { value: Function, default?: () => any }\nThe default key must be a factory that returns the default value for that attribute.\n Streaming Node Output\u200b\nOne of the benefits of using LangGraph is that it is easy to stream output as it's produced by each node.\n What this means is that only one of the downstream edges will be taken, and which one that is depends on the results of the start node.\n"}]

ç»“æœæ˜¯æˆ‘ä»¬çš„èŠå¤©æœºå™¨äººå¯ä»¥ç”¨æ¥å›ç­”é—®é¢˜çš„é¡µé¢æ‘˜è¦ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†å¼€å§‹å®šä¹‰æˆ‘ä»¬çš„å›¾å½¢ã€‚ä»¥ä¸‹å†…å®¹ä¸ç¬¬1éƒ¨åˆ†ç›¸åŒï¼Œåªæ˜¯æˆ‘ä»¬åœ¨LLMä¸Šæ·»åŠ äº†bind\_toolsã€‚è¿™è®©LLMçŸ¥é“å¦‚æœå®ƒæƒ³ä½¿ç”¨æˆ‘ä»¬çš„æœç´¢å¼•æ“ï¼Œå®ƒå¯ä»¥ä½¿ç”¨æ­£ç¡®çš„JSONæ ¼å¼ã€‚

```python
from typing import Annotated

from langchain_anthropic import ChatAnthropic
from typing_extensions import TypedDict

from langgraph.graph import StateGraph
from langgraph.graph.message import add_messages


class State(TypedDict):
    messages: Annotated[list, add_messages]


graph_builder = StateGraph(State)


llm = ChatAnthropic(model="claude-3-haiku-20240307")
# Modification: tell the LLM which tools it can call
llm_with_tools = llm.bind_tools(tools)


def chatbot(state: State):
    return {"messages": [llm_with_tools.invoke(state["messages"])]}


graph_builder.add_node("chatbot", chatbot)
```

æ¥ä¸‹æ¥ï¼Œå¦‚æœå·¥å…·è¢«è°ƒç”¨ï¼Œæˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥å®é™…è¿è¡Œå®ƒä»¬ã€‚æˆ‘ä»¬å°†é€šè¿‡å°†å·¥å…·æ·»åŠ åˆ°æ–°èŠ‚ç‚¹æ¥åšåˆ°è¿™ä¸€ç‚¹ã€‚

ä¸‹é¢ï¼Œå®ç°ä¸€ä¸ªBasicToolNodeï¼Œå®ƒæ£€æŸ¥çŠ¶æ€ä¸­çš„æœ€æ–°æ¶ˆæ¯ï¼Œå¦‚æœæ¶ˆæ¯åŒ…å«tool\_callsï¼Œå®ƒå°†è°ƒç”¨å·¥å…·ã€‚å®ƒä¾èµ–äºLLMçš„tool\_callingæ”¯æŒï¼Œè¿™åœ¨Anthropicã€OpenAIã€è°·æ­ŒåŒå­åº§å’Œè®¸å¤šå…¶ä»–LLMæä¾›å•†ä¸­å¯ç”¨ã€‚

ç¨åæˆ‘ä»¬å°†ç”¨LangGraphçš„é¢„æ„å»ºToolNodeæ›¿æ¢å®ƒä»¥åŠ å¿«é€Ÿåº¦ï¼Œä½†é¦–å…ˆè‡ªå·±æ„å»ºå®ƒå¾ˆæœ‰å¯å‘æ€§ã€‚

```python
import json

from langchain_core.messages import ToolMessage


class BasicToolNode:
    """A node that runs the tools requested in the last AIMessage."""

    def __init__(self, tools: list) -> None:
        self.tools_by_name = {tool.name: tool for tool in tools}

    def __call__(self, inputs: dict):
        if messages := inputs.get("messages", []):
            message = messages[-1]
        else:
            raise ValueError("No message found in input")
        outputs = []
        for tool_call in message.tool_calls:
            tool_result = self.tools_by_name[tool_call["name"]].invoke(
                tool_call["args"]
            )
            outputs.append(
                ToolMessage(
                    content=json.dumps(tool_result),
                    name=tool_call["name"],
                    tool_call_id=tool_call["id"],
                )
            )
        return {"messages": outputs}


tool_node = BasicToolNode(tools=[tool])
graph_builder.add_node("tools", tool_node)
```

æ·»åŠ å·¥å…·èŠ‚ç‚¹åï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰conditional\_edgesã€‚å›æƒ³ä¸€ä¸‹ï¼Œè¾¹å°†æ§åˆ¶æµä»ä¸€ä¸ªèŠ‚ç‚¹è·¯ç”±åˆ°ä¸‹ä¸€ä¸ªèŠ‚ç‚¹ã€‚æ¡ä»¶è¾¹é€šå¸¸åŒ…å«â€œifâ€è¯­å¥ï¼Œæ ¹æ®å½“å‰å›¾å½¢çŠ¶æ€è·¯ç”±åˆ°ä¸åŒçš„èŠ‚ç‚¹ã€‚è¿™äº›å‡½æ•°æ¥æ”¶å½“å‰å›¾å½¢çŠ¶æ€å¹¶è¿”å›ä¸€ä¸ªå­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨ï¼ŒæŒ‡ç¤ºä¸‹ä¸€æ­¥è°ƒç”¨å“ªä¸ªèŠ‚ç‚¹ã€‚

ä¸‹é¢ï¼Œcallå®šä¹‰ä¸€ä¸ªåä¸ºroute\_toolsçš„è·¯ç”±å™¨å‡½æ•°ï¼Œå®ƒæ£€æŸ¥èŠå¤©æœºå™¨äººè¾“å‡ºä¸­çš„tool\_callsã€‚é€šè¿‡è°ƒç”¨add\_conditional\_edgeså°†æ­¤å‡½æ•°æä¾›ç»™å›¾ï¼Œå®ƒå‘Šè¯‰å›¾æ¯å½“èŠå¤©æœºå™¨äººèŠ‚ç‚¹å®Œæˆæ—¶æ£€æŸ¥æ­¤å‡½æ•°ä»¥æŸ¥çœ‹ä¸‹ä¸€æ­¥å»å“ªé‡Œã€‚

å¦‚æœå­˜åœ¨å·¥å…·è°ƒç”¨ï¼Œæ¡ä»¶å°†è·¯ç”±åˆ°å·¥å…·ï¼Œå¦‚æœä¸å­˜åœ¨ï¼Œåˆ™è·¯ç”±åˆ°â€œ\_\_**end\_\_**â€ã€‚

ç¨åï¼Œæˆ‘ä»¬å°†ç”¨é¢„æ„å»ºçš„tools\_conditionæ›¿æ¢å®ƒï¼Œä»¥æ›´åŠ ç®€æ´ï¼Œä½†é¦–å…ˆè‡ªå·±å®ç°å®ƒä¼šä½¿äº‹æƒ…æ›´åŠ æ¸…æ™°ã€‚

```python
from typing import Literal


def route_tools(
    state: State,
) -> Literal["tools", "__end__"]:
    """Use in the conditional_edge to route to the ToolNode if the last message

    has tool calls. Otherwise, route to the end."""
    if isinstance(state, list):
        ai_message = state[-1]
    elif messages := state.get("messages", []):
        ai_message = messages[-1]
    else:
        raise ValueError(f"No messages found in input state to tool_edge: {state}")
    if hasattr(ai_message, "tool_calls") and len(ai_message.tool_calls) > 0:
        return "tools"
    return "__end__"


# The `tools_condition` function returns "tools" if the chatbot asks to use a tool, and "__end__" if
# it is fine directly responding. This conditional routing defines the main agent loop.
graph_builder.add_conditional_edges(
    "chatbot",
    route_tools,
    # The following dictionary lets you tell the graph to interpret the condition's outputs as a specific node
    # It defaults to the identity function, but if you
    # want to use a node named something else apart from "tools",
    # You can update the value of the dictionary to something else
    # e.g., "tools": "my_tools"
    {"tools": "tools", "__end__": "__end__"},
)
# Any time a tool is called, we return to the chatbot to decide the next step
graph_builder.add_edge("tools", "chatbot")
graph_builder.set_entry_point("chatbot")
graph = graph_builder.compile()
```

è¯·æ³¨æ„ï¼Œæ¡ä»¶è¾¹ä»å•ä¸ªèŠ‚ç‚¹å¼€å§‹ã€‚è¿™å‘Šè¯‰å›¾è¡¨â€œä»»ä½•æ—¶å€™â€˜chatbotâ€™èŠ‚ç‚¹è¿è¡Œï¼Œå¦‚æœå®ƒè°ƒç”¨å·¥å…·ï¼Œè¦ä¹ˆè½¬åˆ°â€˜å·¥å…·â€™ï¼Œè¦ä¹ˆå¦‚æœå®ƒç›´æ¥å“åº”ï¼Œåˆ™ç»“æŸå¾ªç¯ã€‚

å’Œé¢„å»ºtools\_conditionä¸€æ ·ï¼Œæˆ‘ä»¬çš„å‡½æ•°å¦‚æœæ²¡æœ‰å·¥å…·è°ƒç”¨å°±è¿”å›â€œ\_\_**end\_\_**â€å­—ç¬¦ä¸²ï¼Œå½“å›¾è¿‡æ¸¡åˆ°\_\_end\_\_æ—¶ï¼Œå®ƒæ²¡æœ‰æ›´å¤šçš„ä»»åŠ¡è¦å®Œæˆï¼Œåœæ­¢æ‰§è¡Œï¼Œå› ä¸ºæ¡ä»¶å¯ä»¥è¿”å›\_\_end\_\_ï¼Œæ‰€ä»¥æˆ‘ä»¬è¿™æ¬¡ä¸éœ€è¦æ˜¾å¼è®¾ç½®finish\_pointï¼Œæˆ‘ä»¬çš„å›¾å·²ç»æœ‰åŠæ³•å®Œæˆäº†ï¼

è®©æˆ‘ä»¬å¯è§†åŒ–æˆ‘ä»¬æ„å»ºçš„å›¾å½¢ã€‚ä»¥ä¸‹å‡½æ•°æœ‰ä¸€äº›é¢å¤–çš„ä¾èµ–é¡¹è¦è¿è¡Œï¼Œè¿™äº›ä¾èµ–é¡¹å¯¹æœ¬æ•™ç¨‹ä¸é‡è¦ã€‚

```python
from IPython.display import Image, display

try:
    display(Image(graph.get_graph().draw_mermaid_png()))
except Exception:
    # This requires some extra dependencies and is optional
    pass
```

<figure><img src="../../.gitbook/assets/ä¸‹è½½ (1) (2).jpeg" alt=""><figcaption></figcaption></figure>

ç°åœ¨æˆ‘ä»¬å¯ä»¥åœ¨è®­ç»ƒæ•°æ®ä¹‹å¤–é—®æœºå™¨äººé—®é¢˜ã€‚

```python
from langchain_core.messages import BaseMessage

while True:
    user_input = input("User: ")
    if user_input.lower() in ["quit", "exit", "q"]:
        print("Goodbye!")
        break
    for event in graph.stream({"messages": [("user", user_input)]}):
        for value in event.values():
            if isinstance(value["messages"][-1], BaseMessage):
                print("Assistant:", value["messages"][-1].content)
```

```
User:  what's langgraph all about?
```

```
Assistant: [{'id': 'toolu_01L1TABSBXsHPsebWiMPNqf1', 'input': {'query': 'langgraph'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}]
Assistant: [{"url": "https://langchain-ai.github.io/langgraph/", "content": "LangGraph is framework agnostic (each node is a regular python function). It extends the core Runnable API (shared interface for streaming, async, and batch calls) to make it easy to: Seamless state management across multiple turns of conversation or tool usage. The ability to flexibly route between nodes based on dynamic criteria."}, {"url": "https://blog.langchain.dev/langgraph-multi-agent-workflows/", "content": "As a part of the launch, we highlighted two simple runtimes: one that is the equivalent of the AgentExecutor in langchain, and a second that was a version of that aimed at message passing and chat models.\n It's important to note that these three examples are only a few of the possible examples we could highlight - there are almost assuredly other examples out there and we look forward to seeing what the community comes up with!\n LangGraph: Multi-Agent Workflows\nLinks\nLast week we highlighted LangGraph - a new package (available in both Python and JS) to better enable creation of LLM workflows containing cycles, which are a critical component of most agent runtimes. \"\nAnother key difference between Autogen and LangGraph is that LangGraph is fully integrated into the LangChain ecosystem, meaning you take fully advantage of all the LangChain integrations and LangSmith observability.\n As part of this launch, we're also excited to highlight a few applications built on top of LangGraph that utilize the concept of multiple agents.\n"}]
Assistant: Based on the search results, LangGraph is a framework-agnostic Python and JavaScript library that extends the core Runnable API from the LangChain project to enable the creation of more complex workflows involving multiple agents or components. Some key things about LangGraph:

- It makes it easier to manage state across multiple turns of conversation or tool usage, and to dynamically route between different nodes/components based on criteria.

- It is integrated with the LangChain ecosystem, allowing you to take advantage of LangChain integrations and observability features.

- It enables the creation of multi-agent workflows, where different components or agents can be chained together in more flexible and complex ways than the standard LangChain AgentExecutor.

- The core idea is to provide a more powerful and flexible framework for building LLM-powered applications and workflows, beyond what is possible with just the core LangChain tools.

Overall, LangGraph seems to be a useful addition to the LangChain toolkit, focused on enabling more advanced, multi-agent style applications and workflows powered by large language models.
```

```
User:  neat!
```

```
Assistant: I'm afraid I don't have enough context to provide a substantive response to "neat!". As an AI assistant, I'm designed to have conversations and provide information to users, but I need more details or a specific question from you in order to give a helpful reply. Could you please rephrase your request or provide some additional context? I'd be happy to assist further once I understand what you're looking for.
```

```
User:  what?
```

```
Assistant: I'm afraid I don't have enough context to provide a meaningful response to "what?". Could you please rephrase your request or provide more details about what you are asking? I'd be happy to try to assist you further once I have a clearer understanding of your query.
```

```
User:  q
```

```
Goodbye!
```

æ­å–œï¼æ‚¨å·²ç»åœ¨langgraphä¸­åˆ›å»ºäº†ä¸€ä¸ªä¼šè¯æ™ºèƒ½ä½“ï¼Œå®ƒå¯ä»¥ä½¿ç”¨æœç´¢å¼•æ“åœ¨éœ€è¦æ—¶æ£€ç´¢æ›´æ–°çš„ä¿¡æ¯ã€‚ç°åœ¨å®ƒå¯ä»¥å¤„ç†æ›´å¹¿æ³›çš„ç”¨æˆ·æŸ¥è¯¢ã€‚è¦æ£€æŸ¥æ‚¨çš„ä»£ç†åˆšåˆšæ‰§è¡Œçš„æ‰€æœ‰æ­¥éª¤ï¼Œè¯·æŸ¥çœ‹LangSmithè·Ÿè¸ªã€‚

æˆ‘ä»¬çš„èŠå¤©æœºå™¨äººä»ç„¶æ— æ³•è‡ªå·±è®°ä½è¿‡å»çš„äº’åŠ¨ï¼Œè¿™é™åˆ¶äº†å®ƒè¿›è¡Œè¿è´¯ã€å¤šè½®å¯¹è¯çš„èƒ½åŠ›ã€‚åœ¨ä¸‹ä¸€éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†æ·»åŠ å†…å­˜æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚

æˆ‘ä»¬åœ¨æœ¬èŠ‚ä¸­åˆ›å»ºçš„å›¾å½¢çš„å®Œæ•´ä»£ç å¦‚ä¸‹æ‰€ç¤ºï¼Œå°†æˆ‘ä»¬çš„BasicToolNodeæ›¿æ¢ä¸ºé¢„æ„å»ºçš„ToolNodeï¼Œå¹¶å°†æˆ‘ä»¬çš„route\_toolsæ¡ä»¶æ›¿æ¢ä¸ºé¢„æ„å»ºçš„tools\_condition

```python
from typing import Annotated

from langchain_anthropic import ChatAnthropic
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.messages import BaseMessage
from typing_extensions import TypedDict

from langgraph.graph import StateGraph
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode, tools_condition


class State(TypedDict):
    messages: Annotated[list, add_messages]


graph_builder = StateGraph(State)


tool = TavilySearchResults(max_results=2)
tools = [tool]
llm = ChatAnthropic(model="claude-3-haiku-20240307")
llm_with_tools = llm.bind_tools(tools)


def chatbot(state: State):
    return {"messages": [llm_with_tools.invoke(state["messages"])]}


graph_builder.add_node("chatbot", chatbot)

tool_node = ToolNode(tools=[tool])
graph_builder.add_node("tools", tool_node)

graph_builder.add_conditional_edges(
    "chatbot",
    tools_condition,
)
# Any time a tool is called, we return to the chatbot to decide the next step
graph_builder.add_edge("tools", "chatbot")
graph_builder.set_entry_point("chatbot")
graph = graph_builder.compile()
```
